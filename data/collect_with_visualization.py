#!/usr/bin/env python3
"""
GitHub Issuesë¥¼ í†µí•´ ìˆ˜ì§‘ëœ ì‚¬ìš©ì ì—°êµ¬ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ì‹œê°í™”í•˜ëŠ” ì™„ì „í•œ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ”:
1. GitHub Issues APIë¥¼ ì‚¬ìš©í•´ ì‚¬ìš©ì ì—°êµ¬ ê²°ê³¼ë¥¼ ìˆ˜ì§‘
2. JSON ë°ì´í„°ë¥¼ íŒŒì‹±í•˜ê³  ì •ë¦¬
3. ì§ˆë¬¸ë³„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¶„ì„
4. ê²°ê³¼ë¥¼ CSVì™€ JSONìœ¼ë¡œ ì €ì¥
5. ê³ ê¸‰ ì‹œê°í™” ë° ì°¨íŠ¸ ìƒì„±
6. ìƒì„¸í•œ í†µê³„ ë¶„ì„ ì œê³µ
"""

import json
import os
import csv
import requests
from datetime import datetime
from collections import defaultdict, Counter
import configparser
from typing import Dict, List, Any
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = ['Arial Unicode MS', 'DejaVu Sans', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

class GitHubResultsVisualizer:
    def __init__(self, token: str, owner: str = "deep-overflow", repo: str = "InterGenEval_user_study"):
        """
        GitHub Issues ê²°ê³¼ ìˆ˜ì§‘ ë° ì‹œê°í™”ê¸° ì´ˆê¸°í™”
        
        Args:
            token: GitHub Personal Access Token
            owner: Repository ì†Œìœ ì
            repo: Repository ì´ë¦„
        """
        self.token = token
        self.owner = owner
        self.repo = repo
        self.headers = {
            'Authorization': f'token {token}',
            'Accept': 'application/vnd.github.v3+json'
        }
        self.base_url = f"https://api.github.com/repos/{owner}/{repo}"
        
        # ì§ˆë¬¸ ì •ë³´
        self.question_names = [
            'interaction_accuracy',
            'entity_accuracy', 
            'temporal_consistency',
            'prompt_faithfulness',
            'overall_quality'
        ]
        
        self.question_labels = {
            'interaction_accuracy': 'ìƒí˜¸ì‘ìš© ì •í™•ì„±',
            'entity_accuracy': 'ëŒ€ìƒ ì •í™•ì„±', 
            'temporal_consistency': 'ì‹œê°„ì  ì¼ê´€ì„±',
            'prompt_faithfulness': 'í”„ë¡¬í”„íŠ¸ ì¶©ì‹¤ë„',
            'overall_quality': 'ì „ë°˜ì  í’ˆì§ˆ'
        }
        
        # ìƒ‰ìƒ íŒ”ë ˆíŠ¸
        self.colors = sns.color_palette("husl", n_colors=8)
        
    def collect_study_results(self) -> List[Dict[str, Any]]:
        """GitHub Issuesì—ì„œ ì‚¬ìš©ì ì—°êµ¬ ê²°ê³¼ë¥¼ ìˆ˜ì§‘"""
        print("ğŸ” Collecting user study results from GitHub Issues...")
        
        url = f"{self.base_url}/issues"
        params = {
            'labels': 'user-study-result',
            'state': 'all',
            'per_page': 100
        }
        
        all_results = []
        page = 1
        
        while True:
            params['page'] = page
            response = requests.get(url, headers=self.headers, params=params)
            
            if response.status_code != 200:
                print(f"âŒ Error fetching issues: {response.status_code}")
                break
                
            issues = response.json()
            if not issues:
                break
                
            print(f"ğŸ“„ Processing page {page} ({len(issues)} issues)...")
            
            for issue in issues:
                try:
                    result = self.parse_issue_result(issue)
                    if result:
                        all_results.append(result)
                except Exception as e:
                    print(f"âš ï¸ Error parsing issue #{issue['number']}: {e}")
                    
            page += 1
            
        print(f"âœ… Collected {len(all_results)} valid study results")
        return all_results
    
    def parse_issue_result(self, issue: Dict[str, Any]) -> Dict[str, Any]:
        """GitHub Issueì—ì„œ ì‚¬ìš©ì ì—°êµ¬ ê²°ê³¼ íŒŒì‹±"""
        body = issue['body']
        
        # Extract JSON data from markdown code block
        json_start = body.find('```json')
        json_end = body.find('```', json_start + 7)
        
        if json_start == -1 or json_end == -1:
            raise ValueError("No JSON data found in issue body")
            
        json_str = body[json_start + 7:json_end].strip()
        
        try:
            result_data = json.loads(json_str)
            result_data['github_issue_number'] = issue['number']
            result_data['github_created_at'] = issue['created_at']
            result_data['github_url'] = issue['html_url']
            return result_data
        except json.JSONDecodeError as e:
            raise ValueError(f"Invalid JSON data: {e}")
    
    def analyze_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ìˆ˜ì§‘ëœ ê²°ê³¼ ë°ì´í„° ë¶„ì„"""
        print("ğŸ“Š Analyzing collected results...")
        
        analysis = {
            'total_participants': len(results),
            'total_comparisons': 0,
            'question_analyses': {},
            'demographics': defaultdict(list),
            'study_durations': [],
            'completion_times': [],
            'raw_comparison_data': []
        }
        
        # Initialize question analyses
        for question in self.question_names:
            analysis['question_analyses'][question] = {
                'model_comparisons': defaultdict(lambda: {'wins': 0, 'total': 0}),
                'comparison_sets': defaultdict(list),
                'participant_choices': []
            }
        
        for result in results:
            # Study duration
            duration_minutes = result.get('studyDuration', 0) / 1000 / 60
            analysis['study_durations'].append(duration_minutes)
            
            # Completion time
            analysis['completion_times'].append(result.get('timestamp'))
            
            # Demographics
            demographics = result.get('demographics', {})
            for key, value in demographics.items():
                analysis['demographics'][key].append(value)
            
            # Process responses
            responses = result.get('responses', {})
            for comparison_set, videos in responses.items():
                analysis['total_comparisons'] += len(videos)
                
                for video_id, response_data in videos.items():
                    # Handle different response formats
                    if isinstance(response_data, dict) and 'answers' in response_data:
                        # New multi-question format
                        answers = response_data['answers']
                        for question in self.question_names:
                            choice = answers.get(question)
                            if choice in ['A', 'B']:
                                self._process_choice(analysis['question_analyses'][question], 
                                                   comparison_set, choice, result, video_id, response_data)
                    elif isinstance(response_data, str):
                        # Legacy single choice format
                        choice = response_data
                        if choice in ['A', 'B']:
                            self._process_choice(analysis['question_analyses']['overall_quality'], 
                                               comparison_set, choice, result, video_id, {'choice': choice})
                    elif isinstance(response_data, dict) and 'choice' in response_data:
                        # Old object format
                        choice = response_data.get('choice')
                        if choice in ['A', 'B']:
                            self._process_choice(analysis['question_analyses']['overall_quality'], 
                                               comparison_set, choice, result, video_id, response_data)
        
        # Calculate win rates
        for question, q_analysis in analysis['question_analyses'].items():
            for model, stats in q_analysis['model_comparisons'].items():
                stats['win_rate'] = stats['wins'] / stats['total'] if stats['total'] > 0 else 0
                
        return analysis
    
    def _process_choice(self, question_analysis, comparison_set, choice, result, video_id, response_data):
        """Helper method to process a single choice"""
        models = comparison_set.split('_vs_')
        if len(models) == 2:
            chosen_model = models[0] if choice == 'A' else models[1]
            other_model = models[1] if choice == 'A' else models[0]
            
            # Record win for chosen model
            question_analysis['model_comparisons'][chosen_model]['wins'] += 1
            question_analysis['model_comparisons'][chosen_model]['total'] += 1
            question_analysis['model_comparisons'][other_model]['total'] += 1
            
            # Store detailed data
            question_analysis['participant_choices'].append({
                'participant': result.get('participantId'),
                'comparison_set': comparison_set,
                'video_id': video_id,
                'choice': choice,
                'chosen_model': chosen_model,
                'model_a': models[0],
                'model_b': models[1],
                'timestamp': response_data.get('timestamp') if isinstance(response_data, dict) else None
            })
    
    def create_comprehensive_visualizations(self, analysis: Dict[str, Any], output_dir: str, timestamp: str):
        """í¬ê´„ì ì¸ ì‹œê°í™” ìƒì„±"""
        print("ğŸ¨ Creating comprehensive visualizations...")
        
        # Set style
        plt.style.use('seaborn-v0_8-whitegrid')
        
        # 1. ì§ˆë¬¸ë³„ ëª¨ë¸ ì„±ëŠ¥ íˆíŠ¸ë§µ
        self._create_performance_heatmap(analysis, output_dir, timestamp)
        
        # 2. ì§ˆë¬¸ë³„ ìƒì„¸ ë°” ì°¨íŠ¸
        self._create_detailed_bar_charts(analysis, output_dir, timestamp)
        
        # 3. ëª¨ë¸ ê°„ ì§ì ‘ ë¹„êµ ë§¤íŠ¸ë¦­ìŠ¤
        self._create_model_comparison_matrix(analysis, output_dir, timestamp)
        
        # 4. ì—°êµ¬ ì°¸ì—¬ í†µê³„
        self._create_participation_stats(analysis, output_dir, timestamp)
        
        # 5. ì¢…í•© ëŒ€ì‹œë³´ë“œ
        self._create_dashboard(analysis, output_dir, timestamp)
        
        print("âœ… All visualizations created!")
    
    def _create_performance_heatmap(self, analysis: Dict[str, Any], output_dir: str, timestamp: str):
        """ì§ˆë¬¸ë³„ ëª¨ë¸ ì„±ëŠ¥ íˆíŠ¸ë§µ ìƒì„±"""
        # ë°ì´í„° ì¤€ë¹„
        models = set()
        for q_analysis in analysis['question_analyses'].values():
            models.update(q_analysis['model_comparisons'].keys())
        
        models = sorted(list(models))
        
        # íˆíŠ¸ë§µ ë°ì´í„° ìƒì„±
        heatmap_data = []
        for question in self.question_names:
            row = []
            q_analysis = analysis['question_analyses'][question]
            for model in models:
                win_rate = q_analysis['model_comparisons'].get(model, {}).get('win_rate', 0)
                row.append(win_rate)
            heatmap_data.append(row)
        
        # íˆíŠ¸ë§µ ìƒì„±
        fig, ax = plt.subplots(figsize=(12, 8))
        
        im = ax.imshow(heatmap_data, cmap='RdYlBu_r', aspect='auto', vmin=0, vmax=1)
        
        # ì¶• ì„¤ì •
        ax.set_xticks(range(len(models)))
        ax.set_yticks(range(len(self.question_names)))
        ax.set_xticklabels(models, rotation=45, ha='right')
        ax.set_yticklabels([self.question_labels[q] for q in self.question_names])
        
        # ê°’ í‘œì‹œ
        for i in range(len(self.question_names)):
            for j in range(len(models)):
                value = heatmap_data[i][j]
                text = ax.text(j, i, f'{value:.2f}', ha='center', va='center',
                             color='white' if value > 0.5 else 'black', fontweight='bold')
        
        # ì»¬ëŸ¬ë°”
        cbar = plt.colorbar(im, ax=ax)
        cbar.set_label('ìŠ¹ë¥  (Win Rate)', rotation=270, labelpad=20)
        
        plt.title('ì§ˆë¬¸ë³„ ëª¨ë¸ ì„±ëŠ¥ íˆíŠ¸ë§µ\nModel Performance Heatmap by Question', 
                 fontsize=16, fontweight='bold', pad=20)
        plt.tight_layout()
        
        heatmap_file = os.path.join(output_dir, f"performance_heatmap_{timestamp}.png")
        plt.savefig(heatmap_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ”¥ Performance heatmap saved to: {heatmap_file}")
    
    def _create_detailed_bar_charts(self, analysis: Dict[str, Any], output_dir: str, timestamp: str):
        """ì§ˆë¬¸ë³„ ìƒì„¸ ë°” ì°¨íŠ¸ ìƒì„±"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()
        
        for i, question in enumerate(self.question_names):
            ax = axes[i]
            q_analysis = analysis['question_analyses'][question]
            model_comparisons = q_analysis['model_comparisons']
            
            if model_comparisons:
                models = list(model_comparisons.keys())
                win_rates = [model_comparisons[model]['win_rate'] for model in models]
                totals = [model_comparisons[model]['total'] for model in models]
                
                # ë°” ì°¨íŠ¸ ìƒì„±
                bars = ax.bar(models, win_rates, color=self.colors[:len(models)], alpha=0.8)
                
                # ê°’ ë¼ë²¨ ì¶”ê°€
                for bar, win_rate, total in zip(bars, win_rates, totals):
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                           f'{win_rate:.2f}\n({total})', ha='center', va='bottom', fontweight='bold')
                
                ax.set_title(f'{self.question_labels[question]}', fontsize=12, fontweight='bold')
                ax.set_ylabel('ìŠ¹ë¥  (Win Rate)')
                ax.set_ylim(0, 1.1)
                ax.tick_params(axis='x', rotation=45)
                ax.grid(True, alpha=0.3)
            else:
                ax.text(0.5, 0.5, 'ë°ì´í„° ì—†ìŒ', ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f'{self.question_labels[question]}', fontsize=12, fontweight='bold')
        
        # ë§ˆì§€ë§‰ ì„œë¸Œí”Œë¡¯ ìˆ¨ê¸°ê¸°
        axes[-1].set_visible(False)
        
        plt.suptitle('ì§ˆë¬¸ë³„ ëª¨ë¸ ì„±ëŠ¥ ìƒì„¸ ë¶„ì„\nDetailed Model Performance by Question', 
                    fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        detailed_file = os.path.join(output_dir, f"detailed_performance_{timestamp}.png")
        plt.savefig(detailed_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š Detailed performance charts saved to: {detailed_file}")
    
    def _create_model_comparison_matrix(self, analysis: Dict[str, Any], output_dir: str, timestamp: str):
        """ëª¨ë¸ ê°„ ì§ì ‘ ë¹„êµ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±"""
        # ëª¨ë¸ë³„ ì „ì²´ ìŠ¹ë¥  ê³„ì‚°
        overall_performance = {}
        
        for question, q_analysis in analysis['question_analyses'].items():
            for model, stats in q_analysis['model_comparisons'].items():
                if model not in overall_performance:
                    overall_performance[model] = {'wins': 0, 'total': 0}
                overall_performance[model]['wins'] += stats['wins']
                overall_performance[model]['total'] += stats['total']
        
        # ìŠ¹ë¥  ê³„ì‚°
        for model in overall_performance:
            stats = overall_performance[model]
            stats['win_rate'] = stats['wins'] / stats['total'] if stats['total'] > 0 else 0
        
        # ì •ë ¬
        sorted_models = sorted(overall_performance.items(), 
                             key=lambda x: x[1]['win_rate'], reverse=True)
        
        # ì‹œê°í™”
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
        
        # ì „ì²´ ìŠ¹ë¥ 
        models = [item[0] for item in sorted_models]
        win_rates = [item[1]['win_rate'] for item in sorted_models]
        totals = [item[1]['total'] for item in sorted_models]
        
        bars1 = ax1.bar(models, win_rates, color=self.colors[:len(models)], alpha=0.8)
        
        for bar, win_rate, total in zip(bars1, win_rates, totals):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{win_rate:.3f}\n({total})', ha='center', va='bottom', fontweight='bold')
        
        ax1.set_title('ì „ì²´ ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„\nOverall Model Performance Ranking', fontweight='bold')
        ax1.set_ylabel('ì „ì²´ ìŠ¹ë¥  (Overall Win Rate)')
        ax1.set_ylim(0, 1.1)
        ax1.tick_params(axis='x', rotation=45)
        ax1.grid(True, alpha=0.3)
        
        # ì§ˆë¬¸ë³„ ì„±ëŠ¥ ë ˆì´ë” ì°¨íŠ¸ (ìƒìœ„ 4ê°œ ëª¨ë¸)
        top_models = models[:4]
        angles = np.linspace(0, 2 * np.pi, len(self.question_names), endpoint=False)
        angles = np.concatenate((angles, [angles[0]]))  # ì›í˜•ìœ¼ë¡œ ë§Œë“¤ê¸°
        
        ax2 = plt.subplot(122, projection='polar')
        
        for model in top_models:
            values = []
            for question in self.question_names:
                q_analysis = analysis['question_analyses'][question]
                win_rate = q_analysis['model_comparisons'].get(model, {}).get('win_rate', 0)
                values.append(win_rate)
            values += [values[0]]  # ì›í˜•ìœ¼ë¡œ ë§Œë“¤ê¸°
            
            ax2.plot(angles, values, 'o-', linewidth=2, label=model)
            ax2.fill(angles, values, alpha=0.25)
        
        ax2.set_xticks(angles[:-1])
        ax2.set_xticklabels([self.question_labels[q] for q in self.question_names])
        ax2.set_ylim(0, 1)
        ax2.set_title('ìƒìœ„ ëª¨ë¸ ì§ˆë¬¸ë³„ ì„±ëŠ¥\nTop Models Performance by Question', 
                     fontweight='bold', pad=20)
        ax2.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        
        plt.tight_layout()
        
        comparison_file = os.path.join(output_dir, f"model_comparison_{timestamp}.png")
        plt.savefig(comparison_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ¯ Model comparison matrix saved to: {comparison_file}")
    
    def _create_participation_stats(self, analysis: Dict[str, Any], output_dir: str, timestamp: str):
        """ì—°êµ¬ ì°¸ì—¬ í†µê³„ ìƒì„±"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # 1. ì—°êµ¬ ì†Œìš” ì‹œê°„ ë¶„í¬
        durations = analysis['study_durations']
        if durations:
            ax1.hist(durations, bins=min(10, len(durations)), color='skyblue', alpha=0.7, edgecolor='black')
            ax1.axvline(np.mean(durations), color='red', linestyle='--', 
                       label=f'í‰ê· : {np.mean(durations):.1f}ë¶„')
            ax1.set_title('ì—°êµ¬ ì†Œìš” ì‹œê°„ ë¶„í¬\nStudy Duration Distribution')
            ax1.set_xlabel('ì‹œê°„ (ë¶„)')
            ax1.set_ylabel('ì°¸ê°€ì ìˆ˜')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
        
        # 2. ì¼ë³„ ì°¸ì—¬ í˜„í™©
        completion_times = [datetime.fromisoformat(t.replace('Z', '+00:00')) for t in analysis['completion_times'] if t]
        if completion_times:
            dates = [t.date() for t in completion_times]
            date_counts = Counter(dates)
            
            ax2.bar(range(len(date_counts)), list(date_counts.values()), color='lightgreen', alpha=0.7)
            ax2.set_title('ì¼ë³„ ì°¸ì—¬ í˜„í™©\nDaily Participation')
            ax2.set_xlabel('ë‚ ì§œ')
            ax2.set_ylabel('ì°¸ê°€ì ìˆ˜')
            ax2.set_xticks(range(len(date_counts)))
            ax2.set_xticklabels([d.strftime('%m/%d') for d in date_counts.keys()], rotation=45)
            ax2.grid(True, alpha=0.3)
        
        # 3. ë¹„êµ ì„¸íŠ¸ë³„ ì°¸ì—¬ë„
        comparison_counts = defaultdict(int)
        for q_analysis in analysis['question_analyses'].values():
            for choice_data in q_analysis['participant_choices']:
                comparison_counts[choice_data['comparison_set']] += 1
        
        if comparison_counts:
            comparison_names = list(comparison_counts.keys())
            comparison_values = list(comparison_counts.values())
            
            ax3.barh(range(len(comparison_names)), comparison_values, color='orange', alpha=0.7)
            ax3.set_title('ë¹„êµ ì„¸íŠ¸ë³„ ì°¸ì—¬ë„\nParticipation by Comparison Set')
            ax3.set_xlabel('í‰ê°€ íšŸìˆ˜')
            ax3.set_yticks(range(len(comparison_names)))
            ax3.set_yticklabels([name.replace('_vs_', ' vs ') for name in comparison_names])
            ax3.grid(True, alpha=0.3)
        
        # 4. ì „ì²´ í†µê³„ ìš”ì•½
        ax4.axis('off')
        stats_text = f"""
        ğŸ“Š ì—°êµ¬ ì°¸ì—¬ í†µê³„ ìš”ì•½
        
        ì´ ì°¸ê°€ì ìˆ˜: {analysis['total_participants']}ëª…
        ì´ í‰ê°€ íšŸìˆ˜: {analysis['total_comparisons']}íšŒ
        í‰ê·  ì—°êµ¬ ì‹œê°„: {np.mean(durations):.1f}ë¶„
        
        ì§ˆë¬¸ë³„ ë°ì´í„°:
        """
        
        for question in self.question_names:
            q_analysis = analysis['question_analyses'][question]
            total_responses = len(q_analysis['participant_choices'])
            stats_text += f"  â€¢ {self.question_labels[question]}: {total_responses}ê°œ ì‘ë‹µ\n"
        
        ax4.text(0.1, 0.7, stats_text, transform=ax4.transAxes, fontsize=12,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
        
        plt.tight_layout()
        
        stats_file = os.path.join(output_dir, f"participation_stats_{timestamp}.png")
        plt.savefig(stats_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“ˆ Participation statistics saved to: {stats_file}")
    
    def _create_dashboard(self, analysis: Dict[str, Any], output_dir: str, timestamp: str):
        """ì¢…í•© ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        fig = plt.figure(figsize=(20, 12))
        
        # ë©”ì¸ ì œëª©
        fig.suptitle('ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ ì‚¬ìš©ì ì—°êµ¬ ê²°ê³¼ ëŒ€ì‹œë³´ë“œ\nVideo Generation Model User Study Results Dashboard', 
                    fontsize=20, fontweight='bold')
        
        # ë ˆì´ì•„ì›ƒ ì„¤ì •
        gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)
        
        # 1. ì „ì²´ ëª¨ë¸ ìˆœìœ„ (ìƒë‹¨ ì¢Œì¸¡)
        ax1 = fig.add_subplot(gs[0, :2])
        overall_performance = {}
        
        for question, q_analysis in analysis['question_analyses'].items():
            for model, stats in q_analysis['model_comparisons'].items():
                if model not in overall_performance:
                    overall_performance[model] = {'wins': 0, 'total': 0}
                overall_performance[model]['wins'] += stats['wins']
                overall_performance[model]['total'] += stats['total']
        
        for model in overall_performance:
            stats = overall_performance[model]
            stats['win_rate'] = stats['wins'] / stats['total'] if stats['total'] > 0 else 0
        
        sorted_models = sorted(overall_performance.items(), key=lambda x: x[1]['win_rate'], reverse=True)
        models = [item[0] for item in sorted_models]
        win_rates = [item[1]['win_rate'] for item in sorted_models]
        
        bars = ax1.bar(models, win_rates, color=self.colors[:len(models)], alpha=0.8)
        ax1.set_title('ì „ì²´ ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„', fontweight='bold')
        ax1.set_ylabel('ìŠ¹ë¥ ')
        ax1.tick_params(axis='x', rotation=45)
        
        # 2. í•µì‹¬ í†µê³„ (ìƒë‹¨ ìš°ì¸¡)
        ax2 = fig.add_subplot(gs[0, 2:])
        ax2.axis('off')
        
        key_stats = f"""
        ğŸ“Š í•µì‹¬ í†µê³„
        
        â€¢ ì´ ì°¸ê°€ì: {analysis['total_participants']}ëª…
        â€¢ ì´ í‰ê°€: {analysis['total_comparisons']}íšŒ
        â€¢ í‰ê·  ì†Œìš”ì‹œê°„: {np.mean(analysis['study_durations']):.1f}ë¶„
        
        ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸:
        â€¢ {models[0]}: {win_rates[0]:.3f}
        
        ğŸ“ í™œìš© ê°€ëŠ¥í•œ ë°ì´í„°:
        â€¢ 5ê°œ ì§ˆë¬¸ë³„ ì„¸ë¶€ ë¶„ì„
        â€¢ ëª¨ë¸ê°„ ì§ì ‘ ë¹„êµ
        â€¢ ì‹œê°„ëŒ€ë³„ ì°¸ì—¬ íŒ¨í„´
        """
        
        ax2.text(0.05, 0.95, key_stats, transform=ax2.transAxes, fontsize=12,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
        
        # 3. ì§ˆë¬¸ë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ (ì¤‘ë‹¨)
        for i, question in enumerate(self.question_names):
            ax = fig.add_subplot(gs[1, i%4]) if i < 4 else fig.add_subplot(gs[2, (i-4)%4])
            
            q_analysis = analysis['question_analyses'][question]
            model_comparisons = q_analysis['model_comparisons']
            
            if model_comparisons:
                top_model = max(model_comparisons.items(), key=lambda x: x[1]['win_rate'])
                model_name, stats = top_model
                
                # ë„ë„› ì°¨íŠ¸
                sizes = [stats['wins'], stats['total'] - stats['wins']]
                colors = ['#ff9999', '#66b3ff']
                
                wedges, texts = ax.pie(sizes, colors=colors, startangle=90)
                
                # ê°€ìš´ë° ì› ì¶”ê°€ (ë„ë„› íš¨ê³¼)
                centre_circle = plt.Circle((0,0), 0.70, fc='white')
                ax.add_artist(centre_circle)
                
                # ì¤‘ì•™ì— ì •ë³´ í‘œì‹œ
                ax.text(0, 0.1, model_name, ha='center', va='center', fontweight='bold', fontsize=10)
                ax.text(0, -0.1, f'{stats["win_rate"]:.3f}', ha='center', va='center', fontsize=12)
                
                ax.set_title(f'{self.question_labels[question]}', fontsize=10, fontweight='bold')
        
        # ë²”ë¡€ ì¶”ê°€
        legend_ax = fig.add_subplot(gs[2, 3])
        legend_ax.axis('off')
        legend_text = """
        ğŸ“‹ ì°¨íŠ¸ ë²”ë¡€
        
        ğŸŸ  ìŠ¹ë¦¬
        ğŸ”µ íŒ¨ë°°
        
        ê° ë„ë„› ì°¨íŠ¸ëŠ” í•´ë‹¹ 
        ì§ˆë¬¸ì—ì„œ ìµœê³  ì„±ëŠ¥ì„ 
        ë³´ì¸ ëª¨ë¸ì„ í‘œì‹œ
        """
        
        legend_ax.text(0.1, 0.8, legend_text, transform=legend_ax.transAxes, fontsize=10,
                      verticalalignment='top', fontfamily='monospace',
                      bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))
        
        dashboard_file = os.path.join(output_dir, f"comprehensive_dashboard_{timestamp}.png")
        plt.savefig(dashboard_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ¯ Comprehensive dashboard saved to: {dashboard_file}")
    
    def save_results(self, results: List[Dict[str, Any]], analysis: Dict[str, Any], output_dir: str = "github_analysis_output"):
        """ê²°ê³¼ë¥¼ ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ê³  ì‹œê°í™” ìƒì„±"""
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ê¸°ë³¸ ê²°ê³¼ ì €ì¥
        raw_file = os.path.join(output_dir, f"raw_results_{timestamp}.json")
        with open(raw_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ Raw results saved to: {raw_file}")
        
        analysis_file = os.path.join(output_dir, f"analysis_summary_{timestamp}.json")
        with open(analysis_file, 'w', encoding='utf-8') as f:
            analysis_json = json.loads(json.dumps(analysis, default=str))
            json.dump(analysis_json, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“Š Analysis summary saved to: {analysis_file}")
        
        # ì‹œê°í™” ìƒì„±
        self.create_comprehensive_visualizations(analysis, output_dir, timestamp)
        
        print(f"\nğŸ‰ Complete analysis package saved to: {output_dir}")

def load_config(config_file='config.ini'):
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    config = configparser.ConfigParser()
    if os.path.exists(config_file):
        config.read(config_file)
        return config
    return None

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ GitHub Issues User Study Results Visualizer")
    print("=" * 60)
    
    # Try to load token from config
    token = None
    config = load_config()
    if config and 'github' in config:
        token = config['github'].get('token')
        if token == 'GITHUB_TOKEN_PLACEHOLDER':
            token = None
    
    if not token:
        token = input("ğŸ”‘ GitHub Personal Access Tokenì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    
    if not token:
        print("âŒ GitHub tokenì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    try:
        # Initialize visualizer
        visualizer = GitHubResultsVisualizer(token)
        
        # Collect results
        results = visualizer.collect_study_results()
        
        if not results:
            print("âŒ ìˆ˜ì§‘ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # Analyze results
        analysis = visualizer.analyze_results(results)
        
        # Print summary
        print("\n" + "="*60)
        print("ğŸ“Š ë¶„ì„ ìš”ì•½:")
        print(f"   ì´ ì°¸ê°€ì ìˆ˜: {analysis['total_participants']}")
        print(f"   ì´ ë¹„êµ íšŸìˆ˜: {analysis['total_comparisons']}")
        if analysis['study_durations']:
            print(f"   í‰ê·  ì—°êµ¬ ì‹œê°„: {np.mean(analysis['study_durations']):.1f}ë¶„")
        
        # Save results and create visualizations
        visualizer.save_results(results, analysis)
        
        print("\nâœ… ë¶„ì„ ë° ì‹œê°í™” ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()